name: Agent Evaluation  # 정기/수동 Agent 품질 평가 파이프라인

on:
  # cron 스케줄에 따라 매일 자정 자동 실행 + 필요 시 수동 실행
  schedule:
    # 매일 자정(UTC 기준) 실행 → KST 기준 오전 9시
    - cron: '0 0 * * *'
  push:
    branches: [main]
    paths:
      - 'agents/**'                     # Agent 변경 시 재평가
      - 'agents/**/tests/evaluation-dataset.json'  # 평가 데이터셋 변경 시
      - 'scripts/run-evaluation.py'     # 평가 스크립트 변경 시
      - '.github/workflows/evaluation-pipeline.yml'  # 워크플로우 파일 변경 시
  workflow_dispatch:
    inputs:
      agent_name:
        description: 'Agent name to evaluate (특정 Agent만 평가하고 싶을 때)'
        required: false
        type: string

env:
  PYTHON_VERSION: '3.11'

jobs:
  evaluate:
    # 하나의 Job에서 전체 평가 사이클(실행 → 리포트 → 비교) 수행
    name: Run Agent Evaluation
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      # AWS 자격 증명 설정 (평가 테스트에서 실제 클라우드 리소스를 사용할 수 있게 함)
      # secrets가 설정되어 있을 때만 실행 (선택적)
      - name: Check AWS credentials availability
        id: check-aws-creds
        run: |
          if [ -n "${{ secrets.AWS_ACCESS_KEY_ID }}" ] && [ -n "${{ secrets.AWS_SECRET_ACCESS_KEY }}" ]; then
            echo "aws_creds_available=true" >> $GITHUB_OUTPUT
          else
            echo "aws_creds_available=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Configure AWS credentials
        if: steps.check-aws-creds.outputs.aws_creds_available == 'true'
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      # 1) 특정 Agent만 평가하거나 2) 모든 Agent를 순회하며 평가 실행
      - name: Run Evaluation
        run: |
          if [ -n "${{ inputs.agent_name }}" ]; then
            # 입력으로 Agent 이름이 주어진 경우: 해당 Agent만 평가
            agent_dir="agents/${{ inputs.agent_name }}"
            dataset_file="$agent_dir/tests/evaluation-dataset.json"
            if [ -f "$dataset_file" ]; then
              python scripts/run-evaluation.py \
                --dataset "$dataset_file" \
                --agent "$agent_dir"
            fi
          else
            # 입력이 없으면 agents/* 아래의 모든 Agent에 대해 평가 실행
            for agent_dir in agents/*/; do
              agent_name=$(basename "$agent_dir")
              dataset_file="$agent_dir/tests/evaluation-dataset.json"
              if [ -f "$dataset_file" ]; then
                echo "Running evaluation for agent: $agent_name"
                python scripts/run-evaluation.py \
                  --dataset "$dataset_file" \
                  --agent "$agent_dir"
              fi
            done
          fi
      
      # 평가 결과(JSON)를 집계하여 요약 마크다운 리포트 생성
      - name: Generate Evaluation Report
        run: |
          python scripts/generate-evaluation-report.py
      
      # 이번 결과와 baseline 결과를 비교하여 성능 향상/저하 여부 확인
      - name: Compare with Baseline
        run: |
          python scripts/compare-evaluation-results.py
      
      # 날짜별로 평가 결과 디렉터리를 아티팩트로 업로드 (장기 추세 분석/복기용)
      - name: Upload Evaluation Results
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-results-$(date +%Y%m%d)
          path: evaluation-results/
          retention-days: 90
